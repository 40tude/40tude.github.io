---
published: true
lang: en-US
layout: default
title: "Understanding the Confusion Matrix"
description: "A gentle, story-driven introduction so youâ€™ll never be confused again."
parent: "Machine Learning"
math: mathjax
date:               2025-12-13 18:00:00 +0000
last_modified_date: 2025-12-13 18:00:00 +0000
---



<!--
TODO
* Do this
* Do that
-->


# Understanding the Confusion Matrix
{: .no_toc }

A gentle, story-driven introduction so youâ€™ll never be confused again.
{: .lead }


<h2 align="center">
<span style="color:orange"><b> ğŸš§ This post is under construction ğŸš§</b></span>
</h2>




<!-- ###################################################################### -->
<!-- ###################################################################### -->
## TL;DR
{: .no_toc }
* For beginners
* In a ML context but applicable elsewhere



<div align="center">
<img src="./assets/img00.webp" alt="" width="600" loading="lazy"/><br/>
<span>Clcik the images to zoom in.</span>
</div>


<!-- ###################################################################### -->
<!-- ###################################################################### -->
## Table of Contents
{: .no_toc .text-delta}
- TOC
{:toc}





<!-- ###################################################################### -->
<!-- ###################################################################### -->
## Introduction

One day, a great Machine Learning philosopher once whispered to me: â€œListen, kid. A Machine Learning project is just like a dish in a fine restaurant. Every step matters, especially the first ones. You can plate it beautifully, serve it with elegance, even impress the criticsâ€¦ but if the recipe is bad, the dish will never be good. And trust me â€” no amount of fancy deployment can save a rotten model. Capiche?â€

<div align="center">
<img src="./assets/img01.webp" alt="" width="600" loading="lazy"/><br/>
<span>RÃ©my, the ML philosopher</span>
</div>

| Step                      | Analogy                        |
|---------------------------|--------------------------------|
| EDA                       | The recipe                     |
| Features Engineering      | The secret sauce               |
| Baseline model            | The first taste                |
| Metrics Analysis          | The critics' score             |
| API & App                 | Sharing with friends           |
| Deployment & Monitoring   | Serve the dish, maintain quality |


At one of the very early steps of the process â€” before jumping into modeling, optimization, and all that fun stuff with SkLearn â€” itâ€™s absolutely crucial to choose a metric, to be able to explain *why* you chose it, to set yourself a goal, and to stick to it. And honestly, thatâ€™s usually the hardest part. Because when we donâ€™t get the results we want, we all have a tendency to â€œbend the dataâ€ until it says what we want to hear â€” and that is a **very, very bad idea**.

When I say â€œchoose a metric,â€ right away you start hearing words like *Recall*, *Precision*, *F1 score*, *Accuracy*â€¦ On top of that, people start talking about the confusion matrix. And thatâ€™s usually where I completely lose my footing.

Letâ€™s be clear: I have no problem with the F1 score itself, or with formulas in general. No, no, it is even worst than that. The real issue was that for a very long time, I just couldnâ€™t wrap my head around how the *labels* in the confusion matrix were written: `TP`, `FP`, `TN`, and `FN`.

Which made itâ€¦ somewhat awkward to properly explain my choices. But that was before. Since then, I went to Lourdes, [saw the light](https://en.wikipedia.org/wiki/Lourdes_apparitions), and now I *almost* understand everything.

So yeah â€” thatâ€™s exactly what weâ€™re going to talk about in this post. As usual, Iâ€™ll start very slowly, without assuming anything about your math background (okay, you still need to know how to add and divide), but by the end â€” pinky swear â€” your ideas will be crystal clear. Youâ€™ll be able to choose and explain the metric for your ML projectâ€¦ and also to legitimately worry if a test tells you that you may have caught this or that disease.

Alright. Letâ€™s get started.








<!-- ###################################################################### -->
<!-- ###################################################################### -->
## The Matrix

To kick things off, I want to finally put to rest this whole â€œhow do I draw a confusion matrix?â€ question.

Letâ€™s imagine we have some â€œthingâ€ that makes predictions. It could be a ML model, a pregnancy test, a fortune tellerâ€¦ whatever you want, itâ€™s your story.

Now, this predictor will sometimes get things right and sometimes get things wrong. If you look closer, you can actually split its predictions into four categories:

1. I said before going into the club that I was going to leave with a girl, and sure enough, I left with the one who became my wife (poor thing, for better or for worse, as they sayâ€¦)
1. I said before going into the club that I was going to leave with a girl, but no luck, I went home alone.
1. I said before going into the club that I wasnâ€™t going to leave with a girl, andâ€¦ I went home alone.
1. I said before going into the club that I wasnâ€™t going to leave with a girl, but the way I danced to those wild beatsâ€¦ I ended up leaving with the most beautiful girl of the night.

Yeah, I know, the example is silly, but thatâ€™s the pointâ€”it sticks in your mind. And trust me, when it comes to ridiculous examples, you havenâ€™t seen anything yet. The worst is yet to comeâ€¦

So, we can sum all this up in a table to evaluate how good the predictions are. If you go clubbing twice a week on average, by the end of the year youâ€™ve made 104 predictionsâ€¦ and now itâ€™s starting to look legit.

Anyway, in the previous paragraph, the key phrase is â€œ**evaluate the accuracy of the predictions**â€. Yeah, I know, thatâ€™s more than one word.

What weâ€™re going to do now is make a two-way table: on one side, you put the predictions, and on the other, you put reality. So itâ€™s a â€œReality vs. Predictionsâ€ matrix, and for now, donâ€™t worry about which is the row and which is the column.

Now, **THE REALLY IMPORTANT THING** is that in each cell of the table, weâ€™ll indicate whether the prediction was correct and what kind of prediction it was.

Letâ€™s clarify the vocabulary:
* The prediction is **NEGATIVE** or **POSITIVE**. Here, a **POSITIVE** prediction means â€œI left with a girl.â€
* Reality is **NEGATIVE** or **POSITIVE**. These are the same â€œunitsâ€ as the predictions so we can compare them.
* The **accuracy of the prediction** compared to reality is **TRUE** or **FALSE**.

Du coup je te propose cette premiÃ¨re matrice vide :

So I suggest this first empty matrix:

```
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   Negative   â”‚          â”‚          â”‚
REALITY       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   Positive   â”‚          â”‚          â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                Negative   Positive
                    PREDICTION
```

Which we will fill in together by announcing what we do â€œout loud.â€

**The data:**
At the end of the year, out of 104 outings, I said I was going to go out with a girl 80 times, but in fact I came home alone 70 times. On the other 24 outings where I said I was going to be serious, I only kept my word 18 times.

Let's continue, and now I say:

1. Prediction **P** and Reality **P**: bottom right. The prediction is correct. I said that I would meet a girl and thatâ€™s what happened (what a charmer!). I write **T** (the prediction was true) and then **P** (because the prediction was **P**). The value is 10 (80â€“70).

2. Prediction **P** and Reality **N**: top right. The prediction is incorrect. I said that I would meet a girl, but I went home alone. I write **F** (the prediction was false) and then **P** (because the prediction was **P**). The value is 70.

3. Prediction **N** and Reality **N**: top left. The prediction is correct. I said that I would behave seriously and go home alone, and that is indeed what happened. I write **T** (the prediction was true) and then **N** (because the prediction was **N**). The value is 18.

4. Prediction **N** and Reality **P**: bottom left. The prediction is incorrect. I said that I would behave seriously and go home alone, but on those nights I met the girl of my life (at least thatâ€™s what I thought at the time). I write **F** (the prediction was false) and then **N** (because the prediction was **N**). The value is 06 (24â€“18).



Tadaa!

```
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   Negative   â”‚  TN 18   â”‚  FP 70   â”‚
REALITY       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   Positive   â”‚  FN 06   â”‚  TP 10   â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                Negative   Positive
                    PREDICTION
```

**Notes:**
* You can see that it doesnâ€™t really matter what is shown in rows or columns. Here I followed what Scikit-Learn displays, a library used with Python, but thatâ€™s really not the most important part. In this case we have:
  * **X-axis (columns)**: what the model predicted (Negative then Positive)
  * **Y-axis (rows)**: the ground truth (Positive at the bottom, Negative at the top)
* Obviously, the sum of all the cells is 104, the total number of nights out.
* In the same way, the sums of the different columns correspond to my predictions (going home alone 24 times, being a charmer 80 times).
* The sum along the main diagonal (top-left, bottom-right) corresponds to the number of correct predictions (with either positive or negative outcomes, but the predictions were correct: 28).
* The sum along the anti-diagonal (bottom-left, top-right) is the number of times the predictions were wrong (76).



**Summary:**
Each cell contains a two-letter code:
- **First letter**: whether the prediction was correct (**T** for True) or wrong (**F** for False)
- **Second letter**: the prediction itself (**P** or **N**)

Building the Matrix Step by Step

| Prediction | Reality | Correct? | Label |
|------------|---------|----------|-------|
| P | P | Yes â†’ T | TP |
| P | N | No â†’ F | FP |
| N | P | No â†’ F | FN |
| N | N | Yes â†’ T | TN |













<!-- ###################################################################### -->
<!-- ###################################################################### -->
## The Metrics

**Precision** = TP / (TP + FP)
"Among everything I predicted as Positive, how many were actually Positive?"

**Recall** (sensitivity) = TP / (TP + FN)
"Among all actual Positives, how many did I find?"




<!-- ###################################################################### -->
<!-- ###################################################################### -->
## Section


<!-- ###################################################################### -->
### Sub-section
{: .no_toc }







<!-- ###################################################################### -->
<!-- ###################################################################### -->
## Conclusion



<!-- ###################################################################### -->
<!-- ###################################################################### -->
## Webliography
